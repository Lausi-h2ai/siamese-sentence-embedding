# siamese-sentence-embedding

This is the code and project report that I wrote for a University Neural Network course.
The basic premise was to implement different machine learning models in order to measure semantic sentence similarity in a labeled dataset.
The three architectures that I implemented and experimented with are as follows:

1. A siamese network architecture that uses BiLSTM and a self-attention block

2. Similar to the first architecture. but also implementing a transformer encoder  block within the siamese paths

3. A more BERT-like approach, where the self-attention block of the previous implementations was removed and only transformer-style multi-head attention is used


For more information, I have also uploaded my project report where I go into more much more detail concerning the models and experiments.